<?xml version="1.0"?>

<!-- 
(c) Copyright, Real-Time Innovations, 2025.  All rights reserved.
RTI grants Licensee a license to use, modify, compile, and create derivative
works of the software solely for use with RTI Connext DDS. Licensee may
redistribute copies of the software provided that all such copies are subject
to this license. The software is provided "as is", with no warranty of any
type, including any warranty for fitness for any purpose. RTI is under no
obligation to maintain or support the software. RTI shall not be liable for
any incidental or consequential damages arising out of the use or inability
to use the software.
-->

<dds xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:noNamespaceSchemaLocation="http://community.rti.com/schema/7.3.0/rti_dds_qos_profiles.xsd">

  <!-- 
    These Variables are intended to be set/calculated in the start_router.sh script. 
    Defaults can be set below. 
  -->

  <configuration_variables>

    <!-- Multicast TTL for over the WAN network -->
    <element>
      <name>WAN_TTL</name>
      <value>6</value>
    </element>

    <!-- Heartbeat Period for RELIABILITY QoS -->
    <element>
      <name>WAN_HB_PERIOD_SEC</name>
      <value>3</value>
    </element>

    <!-- WAN System Timeout -->
    <element>
      <name>WAN_TIMEOUT_SEC</name>
      <value>300</value>
    </element>

    <!-- WAN Max Blocking Time -->
    <element>
      <name>WAN_MAX_BLOCKING_SEC</name>
      <value>15</value>
    </element>

  </configuration_variables>

  <qos_library name="WAN">

    <!--              
    ____________________________________________________________________________
    
                      QOS Profiles used across the WAN/COMMS link       
    ____________________________________________________________________________
    -->

    <qos_profile name="domain_participant_base_qos">

      <!-- 
        This profile is intended for use across a network comms link. 
        It assumes an instance of RTI's Routing Service is on both ends and is using Connext 7.3.0 
        -->
      <domain_participant_qos>


        <!-- Minimizes participant discovery packets -->
        <base_name>
          <element>BuiltinQosSnippetLib::Optimization.Discovery.Participant.Compact</element>
        </base_name>

        <transport_builtin>
          <!-- Select only the UDP transport/ Disable SHMEM -->
          <mask>UDPv4</mask>

          <!-- Set Multicast TTL -->
          <udpv4>
            <multicast_ttl>$(WAN_TTL)</multicast_ttl>
          </udpv4>
        </transport_builtin>


        <discovery_config>
          <!-- 
            Defines amount of messages broadcast on startup to announce a participants presence. 
            DEFAULT: 5.
            This can cause issues when many participants are "announcing" themselves at the same time. 
            -->
          <initial_participant_announcements>3</initial_participant_announcements>

          <!-- 
            Adjust how often applications will assert their presence here per requirements
            These are very lightweight messages but period can be extended to optimize network usage.
            DEFAULT: 30 secs 
            -->
          <participant_liveliness_assert_period>
            <sec>30</sec>
            <!-- nanosec defaults to infinite- ensure a value is always set -->
            <nanosec>0</nanosec>
          </participant_liveliness_assert_period>

          <!-- 
            This is the timeout period by which other applications will consider this to
            be "offline" and remove from internal database.

            Must be > participant_liveliness_assert_period

            If you are getting a lot of timeout's due to lossy comms this can be 
            extended to minimize re-discovery events.

            Adjust per system behavior/requirements

            Currently set to WAN System Timeout

            DEFAULT: 100 secs 
            -->
          <participant_liveliness_lease_duration>
            <sec>$(WAN_TIMEOUT_SEC)</sec>
            <nanosec>0</nanosec>
          </participant_liveliness_lease_duration>

          <!-- Adjust Discovery Heartbeat per RTT of WAN network -->
          <publication_writer>
            <low_watermark>0</low_watermark>
            <high_watermark>1</high_watermark>
            <heartbeat_period>
              <sec>$(WAN_HB_PERIOD_SEC)</sec>
              <nanosec>0</nanosec>
            </heartbeat_period>
            <fast_heartbeat_period>
              <sec>$(WAN_HB_PERIOD_SEC)</sec>
              <nanosec>0</nanosec>
            </fast_heartbeat_period>
            <late_joiner_heartbeat_period>
              <sec>$(WAN_HB_PERIOD_SEC)</sec>
              <nanosec>0</nanosec>
            </late_joiner_heartbeat_period>
            <max_heartbeat_retries>$(WAN_HB_RETRIES)</max_heartbeat_retries>
            <heartbeats_per_max_samples>0</heartbeats_per_max_samples>
            <min_nack_response_delay>
              <sec>0</sec>
              <nanosec>0</nanosec>
            </min_nack_response_delay>
            <max_nack_response_delay>
              <sec>0</sec>
              <nanosec>0</nanosec>
            </max_nack_response_delay>
          </publication_writer>
          <subscription_writer>
            <low_watermark>0</low_watermark>
            <high_watermark>1</high_watermark>
            <heartbeat_period>
              <sec>$(WAN_HB_PERIOD_SEC)</sec>
              <nanosec>0</nanosec>
            </heartbeat_period>
            <fast_heartbeat_period>
              <sec>$(WAN_HB_PERIOD_SEC)</sec>
              <nanosec>0</nanosec>
            </fast_heartbeat_period>
            <late_joiner_heartbeat_period>
              <sec>$(WAN_HB_PERIOD_SEC)</sec>
              <nanosec>0</nanosec>
            </late_joiner_heartbeat_period>
            <max_heartbeat_retries>1000</max_heartbeat_retries>
            <heartbeats_per_max_samples>0</heartbeats_per_max_samples>
            <min_nack_response_delay>
              <sec>0</sec>
              <nanosec>0</nanosec>
            </min_nack_response_delay>
            <max_nack_response_delay>
              <sec>0</sec>
              <nanosec>0</nanosec>
            </max_nack_response_delay>
          </subscription_writer>
          <publication_reader>
            <min_heartbeat_response_delay>
              <sec>0</sec>
              <nanosec>0</nanosec>
            </min_heartbeat_response_delay>
            <max_heartbeat_response_delay>
              <sec>0</sec>
              <nanosec>0</nanosec>
            </max_heartbeat_response_delay>
          </publication_reader>
          <subscription_reader>
            <min_heartbeat_response_delay>
              <sec>0</sec>
              <nanosec>0</nanosec>
            </min_heartbeat_response_delay>
            <max_heartbeat_response_delay>
              <sec>0</sec>
              <nanosec>0</nanosec>
            </max_heartbeat_response_delay>
          </subscription_reader>

        </discovery_config>

        <!-- 
          Disable Type Definitions being propagated across the COMMS link during 
          discovery to optimize bandiwdth usage.
          Enable during integration to make it easier to debug/use Admin Console
        -->
        <resource_limits>
          <type_code_max_serialized_length>0</type_code_max_serialized_length>
          <type_object_max_serialized_length>0</type_object_max_serialized_length>
        </resource_limits>

        <!-- Uncomment below to enable CRC checking for all messages -->
        <!-- 
          <wire_protocol>
            <compute_crc>true</compute_crc>
            <check_crc>true</check_crc>
          </wire_protocol> 
          -->

      </domain_participant_qos>
    </qos_profile>

    <qos_profile name="platform_participant_udpv4_qos" base_name="WAN::domain_participant_base_qos">

      <domain_participant_qos>
        <transport_builtin>
          <mask>UDPv4</mask>

          <udpv6>

            <!-- Adjust Multicast TTL as required using ENV variable -->
            <multicast_ttl>$(WAN_TTL)</multicast_ttl>

            <!-- Increase the receive socket buffer size to improve performance. -->
            <recv_socket_buffer_size>2097152</recv_socket_buffer_size>
          </udpv6>
        </transport_builtin>
        <discovery>
          <initial_peers>
            <!-- 
              Use either unicast or multicast addresses as required.
              Can use host name as well, Connext will attempt to resolve on creation 
              -->
            <!-- <element>192.168.1.5</element> Example IPv4 unicast address -->
            <element>239.255.0.1</element> <!-- Default multicast address -->

          </initial_peers>
        </discovery>
      </domain_participant_qos>

    </qos_profile>

    <qos_profile name="c2_participant_udpv4_qos" base_name="WAN::domain_participant_base_qos">

      <domain_participant_qos>
        <transport_builtin>
          <mask>UDPv4</mask>

          <udpv6>

            <!-- Adjust Multicast TTL as required using ENV variable -->
            <multicast_ttl>$(WAN_TTL)</multicast_ttl>

            <!-- Increase the receive socket buffer size to improve performance. -->
            <recv_socket_buffer_size>2097152</recv_socket_buffer_size>
          </udpv6>
        </transport_builtin>
        <discovery>
          <initial_peers>
            <!-- 
              Use either unicast or multicast addresses as required.
              Can use host name as well, Connext will attempt to resolve on creation 
              -->
            <!-- <element>192.168.1.5</element> Example IPv4 unicast address -->
            <element>239.255.0.1</element> <!-- Default multicast address -->

          </initial_peers>
        </discovery>
      </domain_participant_qos>

    </qos_profile>

    <qos_profile name="platform_participant_udpv6_qos" base_name="WAN::domain_participant_base_qos">

      <domain_participant_qos>
        <transport_builtin>
          <mask>UDPv6</mask>

          <udpv6>

            <!-- Adjust Multicast TTL as required using ENV variable -->
            <multicast_ttl>$(WAN_TTL)</multicast_ttl>

            <!-- Increase the receive socket buffer size to improve performance. -->
            <recv_socket_buffer_size>2097152</recv_socket_buffer_size>
          </udpv6>
        </transport_builtin>
        <discovery>

          <!-- 
              Use either unicast or multicast addresses as required.
              Can use host name as well, Connext will attempt to resolve on creation 
              -->
          <initial_peers>
            <!-- Example IPv6 unicast address: -->
            <!-- <element>FAA0::1</element> -->

            <!-- Example IPv6 loopback address: -->
            <element>::1</element>             
            <element>FF15::1</element> <!-- Example IPv6 multicast address -->

          </initial_peers>
        </discovery>

      </domain_participant_qos>

    </qos_profile>

    <qos_profile name="c2_participant_udpv6_qos" base_name="WAN::domain_participant_base_qos">

      <domain_participant_qos>
        <transport_builtin>
          <mask>UDPv6</mask>

          <udpv6>

            <!-- Adjust Multicast TTL as required using ENV variable -->
            <multicast_ttl>$(WAN_TTL)</multicast_ttl>

            <!-- Increase the receive socket buffer size to improve performance. -->
            <recv_socket_buffer_size>2097152</recv_socket_buffer_size>
          </udpv6>
        </transport_builtin>
        <discovery>

          <!-- 
              Use either unicast or multicast addresses as required.
              Can use host name as well, Connext will attempt to resolve on creation 
              -->
          <initial_peers>
            <!-- Example IPv6 unicast address: -->
            <!-- <element>FAA0::1</element> -->

            <!-- Example IPv6 loopback address: -->
            <element>::1</element> 

            <element>FF15::1</element> <!-- Example IPv6 multicast address -->

          </initial_peers>
        </discovery>

      </domain_participant_qos>

    </qos_profile>

    

    <qos_profile name="status_qos" base_name="BuiltinQosLib::Generic.Common">
      <!-- 
        This profile is intended for Periodic "STATUS" data. 

        Deadline: INF [Default]
        Liveliness: INF [Default]
        -->

      <datawriter_qos>
        <!-- Messages are sent once, does NOT ensure delivery -->
        <reliability>
          <kind>BEST_EFFORT_RELIABILITY_QOS</kind>
        </reliability>

        <!-- 
            How many samples to keep around on a PER INSTANCE basis. 
            BEST_EFFORT only needs 1 as it doesn't resend 
          -->
        <history>
          <kind>KEEP_LAST_HISTORY_QOS</kind>
          <depth>1</depth>
        </history>

      </datawriter_qos>

      <datareader_qos>

        <!-- Messages are sent once, does NOT ensure delivery -->
        <reliability>
          <kind>BEST_EFFORT_RELIABILITY_QOS</kind>
        </reliability>

        <!-- KEEP_LAST will overwrite data if cache is "full" -->
        <history>
          <kind>KEEP_LAST_HISTORY_QOS</kind>
          <depth>50</depth>
        </history>

      </datareader_qos>
    </qos_profile>

    <qos_profile name="event_qos" base_name="BuiltinQosLib::Generic.Common">
      <!-- 
        QoS profile for Event Messages(Commands/CommandAck/ContactReport etc) across the comms link.

        Currently Liveliness(Small heartbeats to maintain "presence") is disabled 
        for bandwidth optimization as well as for Content Filtering use
    
        Liveliness: INF - DISABLED (DEFAULT)
        Deadline: INF - DISABLED (DEFAULT) 
        Durability: VOLATILE - DISABLED (DEFAULT)
        -->

      <datawriter_qos>
        <!-- 
          This spins up a separate thread to send out samples asynchronously
          Commands tend to be larger and including "repairs" can bog down the main write thread
          -->
        <publish_mode>
          <kind>ASYNCHRONOUS_PUBLISH_MODE_QOS</kind>
        </publish_mode>

        <!-- Messages are resent if not delivered -->
        <reliability>
          <kind>RELIABLE_RELIABILITY_QOS</kind>

          <!-- 
            Will block with KEEP_ALL if the queue is "full" from samples not being
            acknowledged 
          -->
          <max_blocking_time>
            <sec>$(WAN_MAX_BLOCKING_SEC)</sec>
            <nanosec>0</nanosec>
          </max_blocking_time>
        </reliability>

        <!-- 
          KEEP_ALL will not overwrite any samples, only removed from queue if 
          acknowledged by ALL readers
        -->
        <history>
          <kind>KEEP_ALL_HISTORY_QOS</kind>
        </history>

        <protocol>

          <rtps_reliable_writer>

            <!-- 
              Will give up on needing acknowledgement per reader if no response received from
              reader after $WAN_TIMEOUT_SEC

              Will still send messages but will be BEST_EFFORT
              -->
            <max_heartbeat_retries>$(WAN_HB_RETRIES)</max_heartbeat_retries>

            <!-- Adjust Heartbeat per RTT of WAN network -->
            <low_watermark>0</low_watermark>
            <high_watermark>1</high_watermark>
            <heartbeat_period>
              <sec>$(WAN_HB_PERIOD_SEC)</sec>
              <nanosec>0</nanosec>
            </heartbeat_period>
            <fast_heartbeat_period>
              <sec>$(WAN_HB_PERIOD_SEC)</sec>
              <nanosec>0</nanosec>
            </fast_heartbeat_period>
            <late_joiner_heartbeat_period>
              <sec>$(WAN_HB_PERIOD_SEC)</sec>
              <nanosec>0</nanosec>
            </late_joiner_heartbeat_period>
            <heartbeats_per_max_samples>0</heartbeats_per_max_samples>
            <min_nack_response_delay>
              <sec>0</sec>
              <nanosec>0</nanosec>
            </min_nack_response_delay>
            <max_nack_response_delay>
              <sec>0</sec>
              <nanosec>0</nanosec>
            </max_nack_response_delay>

          </rtps_reliable_writer>
        </protocol>

      </datawriter_qos>

      <!-- Data Reader QOS -->
      <datareader_qos>

        <!-- Messages are resent if not delivered -->
        <reliability>
          <kind>RELIABLE_RELIABILITY_QOS</kind>
        </reliability>

        <!-- 
          Queue for samples being provided to the application

          KEEP_ALL will not overwrite older messages
          -->
        <history>
          <kind>KEEP_ALL_HISTORY_QOS</kind>
        </history>

        <!-- 
          This increases the buffer of messages before they are fully received 
          FIFO and provided to the application.

          In scenarios with high latency/long RTT(Round Trip Time) it allows for
          missing dropped samples.

          Default: 256
          -->
        <protocol>
          <rtps_reliable_reader>
            <receive_window_size>512</receive_window_size>

          </rtps_reliable_reader>
        </protocol>

      </datareader_qos>

    </qos_profile>


  </qos_library>

</dds>